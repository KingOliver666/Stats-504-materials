iNaturalist data

iNaturalist is a platform that records observations of living organisms, annotated with their locations and taxonomic identifications. We will be considering data from two classes of plants in this notebook. Our goal here is to illustrate the concept of "large scale inference", to study the presence of systematic changes in species locations over time.

```{r}
library(readr)
library(dplyr)
library(tidyr)
library(locfdr)
library(roll)
library(ggplot2)
```

Select a class of plants to analyze.

```{r}
#pclass = "Pinopsida"
pclass = "Polypodiopsida"

pa = sprintf("/home/kshedden/data/Teaching/inaturalist/Plantae_%s.csv.gz", pclass)
```

Below we load the data and construct a time variable that starts on January 1, 2015 and counts in 1000's of days from that origin. In the analysis conducted below, we will be interested in the evidence that for each specific species, the average latitude changes linearly as a function of this 'day' variable.

```{r}
da = read_csv(pa)
da = da %>% mutate(eventDate = as.Date(eventDate))
da = da %>% filter(eventDate >= as.Date('2015-01-01'))
da = da %>% select(!elevation) %>% drop_na()
da = da %>% mutate(date1 = as.Date('2015-01-01'))
da = da %>% mutate(day=difftime(eventDate, date1, units="days")/1000)
```

```{r}
dn = da %>% group_by(species) %>% summarize(n=n())
dn = dn %>% mutate(logn=log10(n))
plt = ggplot(dn, aes(x=logn)) + geom_histogram()
print(plt)
```

Below we calculate the mean latitude for each species.  Individual plants within each species occur at various locations, but each species has a geographic centroid.  We compute the centroid for latitude below since that is our focus here.

```{r}
dm = da %>% group_by(species) %>% summarize(species_latitude=mean(decimalLatitude))
da = inner_join(da, dm, by="species")
head(da)
```
Below we calculate the intraclass correlation coefficient (ICC) of the latitude values, which describes the extent to which different species cluster by latitude.  An ICC of 1 corresponds to complete clustering and an ICC of 0 corresponds to no clustering.

```{r}
var(da$species_latitude) / var(da$decimalLatitude)
```

We will analyze longitude as a circular variable.

```{r}
da = da %>% mutate(lonrad=pi*decimalLongitude/180)
da = da %>% mutate(lonrad_sin=sin(lonrad), lonrad_cos=cos(lonrad))
```

Below we create a synthetic variable that cannot contain any new information.  It will be used below as a newgtive control to evaluate the performance of inference procedures.

```{r}
da = da %>% mutate(fake=lonrad_cos+rnorm(length(lonrad_cos)))
```

Below we fit a linear model for each species that predicts latitude from the time variable called "day" (the number of days since the time origin) and some other control variables. The main interest here is the relationship between "day" and the conditional mean latitude of a species. If the slope of day on latitude is positive for a given species, this species is identified at more northerly locations as time progresses. If the coefficient is negative the species is identified at more southerly locations as time progresses. We assess these effects using two models. The first model has only main effects and the second model allows the time trend in mean latitude to vary by longitude.

```{r}
f = function(x, y) {
  species = as.character(y)
  if (nrow(x) < 10) {
    return(list(species=species, n=nrow(x)))
  }
  
  m1 = lm(decimalLatitude ~ day + lonrad_sin + lonrad_cos + fake, data=x)
  m2 = lm(decimalLatitude ~ day * (lonrad_sin + lonrad_cos + fake), data=x)

  lrt = anova(m1, m2)
  stat = lrt$F[[2]]
  dof = lrt$Df[[2]]
  if (dof == 3) {
    # The log likelihood ratio statistics is chi^2 under the null.  We use the 
    # Wilson and Hilferty transformation to transform it from chi-square to Gaussian.
    lrt_z = (stat / dof)^(1/3) - (1 - 2/(9*dof))
    lrt_z = lrt_z / sqrt(2/(9*dof))
  } else {
    lrt_z = NA
  }
  
  s = sqrt(diag(vcov(m1)))
  day_slope = coef(m1)[["day"]]
  day_slope_se = s[["day"]]
  fake_slope = coef(m1)[["fake"]]
  fake_slope_se = s[["fake"]]

  return(list(species=species, n=nrow(x), lrt_z=lrt_z, 
              day_slope=day_slope, day_slope_se=day_slope_se,
              fake_slope=fake_slope, fake_slope_se=fake_slope_se))
}

rr = da %>% group_by(species) %>% group_map(f) %>% bind_rows()
```

Below we construct T-scores for parameters of interest. For species with large sample sizes these T-scores should also be approximate Z-scores (they approximately follow a standard normal distribution under the null hypothesis that the day slope is zero). For smaller sample sizes we need to account for the uncertainty in the scale parameter estimate induced by the limited degrees of freedom.

```{r}
rr = rr %>% mutate(day_slope_t=day_slope/day_slope_se, fake_slope_t=fake_slope/fake_slope_se)
```

```{r}
rr = left_join(rr, dm, by="species")
```

Account for finite group sizes, by mapping the t-distributed statistics to normally distributed statistics.

```{r}
rr = rr %>% mutate(day_slope_z=qnorm(pt(day_slope_t, n-5)))
rr = rr %>% mutate(fake_slope_z=qnorm(pt(fake_slope_t, n-5)))
```

```{r}
plt = ggplot(rr, aes(x=day_slope_t, y=day_slope_z)) + geom_point()
print(plt)
```

```{r}
for (vn in c("day_slope_z", "fake_slope_z", "lrt_z")) {
  plt = ggplot(rr, aes(sample=.data[[vn]])) + stat_qq(na.rm=T) + geom_abline(intercept=0, slope=1) + ylim(-1, 4)
  print(plt)
}

```
The value calculated below is the threshold needed to control the family-wise error rate (FWER) at 0.05.

```{r}
n = length(is.finite(rr$day_slope_z))
bonf_z = qnorm(1 - 0.025/n)
bonf_z
```

Below we calculate the local FDR for the day slope estimates.

```{r}
zz = rr$day_slope_z
ii = is.finite(zz)
zz = zz[ii]
lfdr = locfdr(zz)
rr$locfdr = NA
rr$locfdr[ii] = zz
```

```{r}
rr1 = rr %>% select(species_latitude, day_slope_z) %>% drop_na() %>% arrange(species_latitude)
rr1$y1 = roll_quantile(rr1$day_slope_z, width=150, p=0.1)
rr1$y2 = roll_quantile(rr1$day_slope_z, width=150, p=0.9)
rr1$z1 = qnorm(0.1)
rr1$z2 = qnorm(0.9)

plt = ggplot(rr1, aes(x=species_latitude, y=day_slope_z)) + geom_point() 
plt = plt + geom_line(aes(x=species_latitude, y=y1, color="orange"))
plt = plt + geom_line(aes(x=species_latitude, y=y2, color="orange"))
plt = plt + geom_line(aes(x=species_latitude, y=z1, color="purple"))
plt = plt + geom_line(aes(x=species_latitude, y=z2, color="purple"))
plt = plt + theme(legend.position="none")
print(plt)
```

Next we plot the day slope Z-score against the sample size. If we are mainly limited by power then the larger Z-scores will be concentrated where the sample size is larger. This plot makes it clear that there are some Z-scores falling far outside the likely range for a standard normal variable, and these values can be either positive or negative. Most of the largest Z-scores (in magnitude) occur with the larger sample sizes.

```{r}
rr1 = rr %>% select(n, day_slope_z) %>% drop_na() %>% arrange(n)
rr1 = rr1 %>% mutate(logn = log(n))
rr1$y1 = roll_quantile(rr1$day_slope_z, width=150, p=0.1)
rr1$y2 = roll_quantile(rr1$day_slope_z, width=150, p=0.9)
rr1$z1 = qnorm(0.1)
rr1$z2 = qnorm(0.9)

plt = ggplot(rr1, aes(x=logn, y=day_slope_z)) + geom_point()
plt = plt + geom_line(aes(x=logn, y=y1, color="orange"))
plt = plt + geom_line(aes(x=logn, y=y2, color="orange"))
plt = plt + geom_line(aes(x=logn, y=z1, color="purple"))
plt = plt + geom_line(aes(x=logn, y=z2, color="purple"))
plt = plt + theme(legend.position="none")
print(plt)
```
Another way to assess whether the Z-scores with greater magnitude concentrate in the species with larger sample sizes is to smooth the absolute Z-score against the log sample size as below.  Doing this, we loose the ability to distinguish positive from negative Z-scores, but we do see a clear tendency for the largest Z-scores to occur in the species with the greatest number of observations.

```{r}
plt = ggplot(rr1, aes(x=logn, y=abs(day_slope_z))) + geom_smooth()
plt = plt + geom_hline(yintercept=sqrt(2/pi))
plt = plt + scale_y_continuous(expand = c(0, 0), limits = c(0, NA))
print(plt)
```
